{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AAML_final_yehtest.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dKioeo5P-c8m","executionInfo":{"status":"ok","timestamp":1641889537703,"user_tz":-480,"elapsed":22767,"user":{"displayName":"葉昭宏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggn_HbAEQbF0F8H3LZDmhDbywgSUiJSt582sOu23A=s64","userId":"07106977847402479880"}},"outputId":"608af8f5-54fb-4e79-c615-183d615c8d65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from tqdm import tqdm"],"metadata":{"id":"aA3-TxwkuS6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldfUaRvpp8D3"},"outputs":[],"source":["DATADIR = '/content/drive/Shareddrives/AAML_final/Training_Dataset'\n","CATEGORIES = ['Fire', 'NoFire']"]},{"cell_type":"code","source":["IMG_SIZE = 64\n","def create_training_data():\n","    training_data = []\n","    for category in CATEGORIES:  \n","\n","        path = os.path.join(DATADIR,category) \n","        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=C 1=O\n","\n","        for img in tqdm(os.listdir(path)):  # iterate over each image\n","            try:\n","                img_array = cv2.imread(os.path.join(path,img))  # convert to array\n","                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n","                training_data.append([new_array, class_num])  # add this to our training_data\n","            except Exception as e:  # in the interest in keeping the output clean...\n","                pass\n","              \n","    return training_data"],"metadata":{"id":"GOj1L2uOwPrW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data = create_training_data()"],"metadata":{"id":"NKLwnnEqwTfl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641889676110,"user_tz":-480,"elapsed":128175,"user":{"displayName":"葉昭宏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggn_HbAEQbF0F8H3LZDmhDbywgSUiJSt582sOu23A=s64","userId":"07106977847402479880"}},"outputId":"7379dce9-24d8-44b4-eab4-2ff79e1fc84d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1124/1124 [00:46<00:00, 24.17it/s]\n","100%|██████████| 1301/1301 [01:14<00:00, 17.40it/s]\n"]}]},{"cell_type":"code","source":["import random\n","\n","print(len(training_data))\n","random.shuffle(training_data)\n","for sample in training_data[:10]:\n","    print(sample[1])"],"metadata":{"id":"pcM6GdZbwVy0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = []\n","Y = []\n","\n","for features,label in training_data:\n","    X.append(features)\n","    Y.append(label)\n","\n","X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n","X = X/255.0\n","Y = np.array(Y)\n","X.shape[1:]"],"metadata":{"id":"txSP6JG-wX70"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Hyperparameter**"],"metadata":{"id":"-kaCm9Dm0vVW"}},{"cell_type":"code","source":["BATCHSIZE_ = 32\n","EPOCHS_ = 100\n","VALIDATION_SPLIT_ = 0.3"],"metadata":{"id":"j99duR050GvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # set up image augmentation\n","# from keras.preprocessing.image import ImageDataGenerator\n","\n","# datagen = ImageDataGenerator(\n","#     rotation_range=15,\n","#     horizontal_flip=True,\n","#     width_shift_range=0.1,\n","#     height_shift_range=0.1\n","#     #zoom_range=0.3\n","#     )\n","# datagen.fit(X)"],"metadata":{"id":"UDxV7pGQwbmk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, AveragePooling2D\n","\n","model = Sequential()\n","\n","\n","model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=X.shape[1:]))\n","model.add(AveragePooling2D())\n","model.add(Dropout(0.5))\n","\n","model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n","model.add(AveragePooling2D())\n","model.add(Dropout(0.5))\n","\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n","model.add(AveragePooling2D())\n","model.add(Dropout(0.5))\n","\n","model.add(Flatten())\n","\n","model.add(Dense(units=256, activation='relu'))\n","model.add(Dropout(0.2))\n","\n","model.add(Dense(units=128, activation='relu'))\n","\n","model.add(Dense(units=2, activation = 'softmax'))\n","\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","#describe original model\n","model.summary()"],"metadata":{"id":"DjshV3KKwcMZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641889704506,"user_tz":-480,"elapsed":3900,"user":{"displayName":"葉昭宏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggn_HbAEQbF0F8H3LZDmhDbywgSUiJSt582sOu23A=s64","userId":"07106977847402479880"}},"outputId":"43f3f06d-a612-4412-fb2e-8b4a9655b031"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 62, 62, 16)        448       \n","                                                                 \n"," average_pooling2d (AverageP  (None, 31, 31, 16)       0         \n"," ooling2D)                                                       \n","                                                                 \n"," dropout (Dropout)           (None, 31, 31, 16)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 29, 29, 32)        4640      \n","                                                                 \n"," average_pooling2d_1 (Averag  (None, 14, 14, 32)       0         \n"," ePooling2D)                                                     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 14, 14, 32)        0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 12, 12, 64)        18496     \n","                                                                 \n"," average_pooling2d_2 (Averag  (None, 6, 6, 64)         0         \n"," ePooling2D)                                                     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 6, 6, 64)          0         \n","                                                                 \n"," flatten (Flatten)           (None, 2304)              0         \n","                                                                 \n"," dense (Dense)               (None, 256)               590080    \n","                                                                 \n"," dropout_3 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 128)               32896     \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 258       \n","                                                                 \n","=================================================================\n","Total params: 646,818\n","Trainable params: 646,818\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["history = model.fit(X, Y, batch_size=32, epochs=100, validation_split=0.3)\n","# model.fit_generator(datagen.flow(X, Y, batch_size=32),\n","#                     epochs=100,\n","#                     verbose=1)"],"metadata":{"id":"Wd67WxPPweYo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('/content/drive/Shareddrives/AAML_final/FireNet-LightWeight-Network-for-Fire-Detection/Trained_Model_FIreNet/train_original.h5')"],"metadata":{"id":"D33oiIAKwi2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"metadata":{"id":"MfcfoWXNwkXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"metadata":{"id":"_JS-Qx47wlyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.utils import plot_model\n","plot_model(model, to_file='model_small.png', show_layer_names=False, show_shapes=True)"],"metadata":{"id":"DFyB40_pw6DZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Above this line,it's the peocedure of baseline\n","\n","\n","and it's time to quantize model\n"],"metadata":{"id":"ONmc1i4uW8Pi"}},{"cell_type":"markdown","source":["**Quantization_model**"],"metadata":{"id":"sTXWR4YDpLzz"}},{"cell_type":"code","source":["!pip install -q tensorflow-model-optimization\n","import tensorflow_model_optimization as tfmot\n","import tensorflow as tf\n","base_model = tf.keras.models.load_model('/content/drive/Shareddrives/AAML_final/FireNet-LightWeight-Network-for-Fire-Detection/Trained_Model_FIreNet/all_prun_0_80.h5')"],"metadata":{"id":"6Pc5hMwspIM4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641882730368,"user_tz":-480,"elapsed":2758,"user":{"displayName":"葉昭宏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggn_HbAEQbF0F8H3LZDmhDbywgSUiJSt582sOu23A=s64","userId":"07106977847402479880"}},"outputId":"e0e8f343-98fd-4117-a92f-6b13825c078b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]}]},{"cell_type":"code","source":["import tensorflow_model_optimization as tfmot\n","\n","quantize_model = tfmot.quantization.keras.quantize_model #make quantization model\n","\n","num_images = X.shape[0] * (1 - VALIDATION_SPLIT_)\n","end_step = np.ceil(num_images / BATCHSIZE_).astype(np.int32) * EPOCHS_\n","\n","#Define model for quantization\n","q_aware_model = quantize_model(base_model)\n","\n","#Recompile part\n","q_aware_model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","#q_aware_model.summary()"],"metadata":{"id":"2rrhalXKpaWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","quantized_tflite_model = converter.convert()\n","import tempfile\n","_, quanted_tflite_file = tempfile.mkstemp('.tflite')\n","\n","with open(quanted_tflite_file, 'wb') as f:\n","  f.write(quantized_tflite_model)\n","\n","print('Saved quanted TFLite model to:', quanted_tflite_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXZTxEYFrK9y","executionInfo":{"status":"ok","timestamp":1641880185562,"user_tz":-480,"elapsed":7054,"user":{"displayName":"葉昭宏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggn_HbAEQbF0F8H3LZDmhDbywgSUiJSt582sOu23A=s64","userId":"07106977847402479880"}},"outputId":"18895d9a-9f10-47ad-877c-fd6558135b84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 55). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /tmp/tmp5h22sue8/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /tmp/tmp5h22sue8/assets\n","WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"]},{"output_type":"stream","name":"stdout","text":["Saved quanted TFLite model to: /tmp/tmptls0i0t8.tflite\n"]}]},{"cell_type":"code","source":["import os\n","\n","q_aware_model.fit(X, Y,\n","           batch_size=BATCHSIZE_, epochs=EPOCHS_, validation_split=VALIDATION_SPLIT_)\n","\n","quant_aware_model.fit(X, Y,\n","           batch_size=BATCHSIZE_, epochs=EPOCHS_, validation_split=VALIDATION_SPLIT_)\n","#evaluate accuracy\n","_, baseline_model_accuracy = model.evaluate(\n","    X, Y, verbose=0)\n","_, q_aware_model_accuracy = q_aware_model.evaluate(\n","   X, Y, verbose=0)"],"metadata":{"id":"pwyM0HwJuJ-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Baseline test accuracy:', baseline_model_accuracy)\n","print('Quant test accuracy:', q_aware_model_accuracy)\n","print('Quant_Dense test accuracy:', quant_aware_model_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DLzfsC1kRoMi","executionInfo":{"status":"ok","timestamp":1641650944192,"user_tz":-480,"elapsed":283,"user":{"displayName":"葉昭宏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggn_HbAEQbF0F8H3LZDmhDbywgSUiJSt582sOu23A=s64","userId":"07106977847402479880"}},"outputId":"fc991bf9-d10a-4fb6-a037-397ebf427a5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline test accuracy: 0.9768881797790527\n","Quant test accuracy: 0.9917457699775696\n","Quant_Dense test accuracy: 0.99092036485672\n"]}]},{"cell_type":"markdown","source":["Baseline test accuracy: 0.9768881797790527\n","Quant test accuracy: 0.985555112361908\n","Quant_Conv2D test accuracy: 0.9880313873291016\n","\n","Baseline test accuracy: 0.9768881797790527\n","Quant test accuracy: 0.9917457699775696\n","Quant_Dense test accuracy: 0.99092036485672"],"metadata":{"id":"H5fzThaHstCA"}},{"cell_type":"markdown","source":["**Custom bit part**"],"metadata":{"id":"rKHhDx7Uxzue"}},{"cell_type":"code","source":["!pip install -q tensorflow-model-optimization\n","import tensorflow_model_optimization as tfmot\n","import tensorflow as tf"],"metadata":{"id":"v1ntx-DuEPDA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set default quantization configuration\n","LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n","MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n","\n","class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n","    # Configure how to quantize weights.\n","    def get_weights_and_quantizers(self, layer):\n","      return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\n","\n","    # Configure how to quantize activations.\n","    def get_activations_and_quantizers(self, layer):\n","      return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n","\n","    def set_quantize_weights(self, layer, quantize_weights):\n","      # Add this line for each item returned in `get_weights_and_quantizers`\n","      # , in the same order\n","      layer.kernel = quantize_weights[0]\n","\n","    def set_quantize_activations(self, layer, quantize_activations):\n","      # Add this line for each item returned in `get_activations_and_quantizers`\n","      # , in the same order.\n","      layer.activation = quantize_activations[0]\n","\n","    # Configure how to quantize outputs (may be equivalent to activations).\n","    def get_output_quantizers(self, layer):\n","      return []\n","\n","    def get_config(self):\n","      return {}"],"metadata":{"id":"uXFLZo29ELkr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["''' This part is made for modify default quantizeconfig '''\n","quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n","quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n","quantize_scope = tfmot.quantization.keras.quantize_scope\n","\n","'''class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\n","    # Edit bit part\n","    # Configure weights to quantize with 4-bit instead of 8-bits.\n","    def get_weights_and_quantizers(self, layer):\n","      return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\n","    \n","    # Quantize specific part\n","    def get_activations_and_quantizers(self, layer):\n","      # Skip quantizing activations.\n","      return []\n","\n","    def set_quantize_activations(self, layer, quantize_activations):\n","      # Empty since `get_activaations_and_quantizers` returns\n","      # an empty list.\n","      return'''"],"metadata":{"id":"CQqsgH3ixywU","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1641882796209,"user_tz":-480,"elapsed":283,"user":{"displayName":"葉昭宏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggn_HbAEQbF0F8H3LZDmhDbywgSUiJSt582sOu23A=s64","userId":"07106977847402479880"}},"outputId":"52301112-836c-47ce-dea7-b17a5f3bee73"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\\n    # Edit bit part\\n    # Configure weights to quantize with 4-bit instead of 8-bits.\\n    def get_weights_and_quantizers(self, layer):\\n      return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\\n    \\n    # Quantize specific part\\n    def get_activations_and_quantizers(self, layer):\\n      # Skip quantizing activations.\\n      return []\\n\\n    def set_quantize_activations(self, layer, quantize_activations):\\n      # Empty since `get_activaations_and_quantizers` returns\\n      # an empty list.\\n      return'"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["def apply_quantization_to_specific(layer):\n","  if isinstance(layer, tf.keras.layers.Dense): #is dense(or other type) layer or not\n","    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n","  return layer\n","\n","annotated_model = tf.keras.models.clone_model(\n","    base_model,\n","    clone_function=apply_quantization_to_specific,\n",")\n","# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n","with quantize_scope(\n","  {'DefaultDenseQuantizeConfig': DefaultDenseQuantizeConfig}):\n","  # Use `quantize_apply` to actually make the model quantization aware.\n","  quant_modified_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n","quant_modified_aware_model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","quant_modified_aware_model.summary()"],"metadata":{"id":"7eSr498zDook","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641882980491,"user_tz":-480,"elapsed":711,"user":{"displayName":"葉昭宏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggn_HbAEQbF0F8H3LZDmhDbywgSUiJSt582sOu23A=s64","userId":"07106977847402479880"}},"outputId":"2a6c9e34-86f0-4266-fdc8-604ded379fc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 62, 62, 16)        448       \n","                                                                 \n"," average_pooling2d (AverageP  (None, 31, 31, 16)       0         \n"," ooling2D)                                                       \n","                                                                 \n"," dropout (Dropout)           (None, 31, 31, 16)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 29, 29, 32)        4640      \n","                                                                 \n"," average_pooling2d_1 (Averag  (None, 14, 14, 32)       0         \n"," ePooling2D)                                                     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 14, 14, 32)        0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 12, 12, 64)        18496     \n","                                                                 \n"," average_pooling2d_2 (Averag  (None, 6, 6, 64)         0         \n"," ePooling2D)                                                     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 6, 6, 64)          0         \n","                                                                 \n"," quant_flatten (QuantizeWrap  (None, 2304)             1         \n"," perV2)                                                          \n","                                                                 \n"," quant_dense (QuantizeWrappe  (None, 256)              590085    \n"," rV2)                                                            \n","                                                                 \n"," quant_dropout_3 (QuantizeWr  (None, 256)              1         \n"," apperV2)                                                        \n","                                                                 \n"," quant_dense_1 (QuantizeWrap  (None, 128)              32901     \n"," perV2)                                                          \n","                                                                 \n"," quant_dense_2 (QuantizeWrap  (None, 2)                263       \n"," perV2)                                                          \n","                                                                 \n","=================================================================\n","Total params: 646,835\n","Trainable params: 646,818\n","Non-trainable params: 17\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["**Modified part result**"],"metadata":{"id":"-SFtHErxPSa9"}},{"cell_type":"code","source":["quant_modified_aware_model.fit(X, Y,\n","           batch_size=BATCHSIZE_, epochs=EPOCHS_, validation_split=VALIDATION_SPLIT_)\n","_, quant_modified_aware_model_accuracy = quant_modified_aware_model.evaluate(\n","   X, Y, verbose=0)\n","print('Quant_bit test accuracy:', quant_modified_aware_model_accuracy)"],"metadata":{"id":"zjUNsGuoKUt0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#converter = tf.lite.TFLiteConverter.from_keras_model(quant_modified_aware_model)\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","quantized_tflite_model = converter.convert()\n","import tempfile\n","_, quanted_tflite_file = tempfile.mkstemp('.tflite')\n","\n","with open(quanted_tflite_file, 'wb') as f:\n","  f.write(quantized_tflite_model)\n","\n","print('Saved quanted TFLite model to:', quanted_tflite_file)"],"metadata":{"id":"e6V5f3ggmR4N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641889735635,"user_tz":-480,"elapsed":3792,"user":{"displayName":"葉昭宏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggn_HbAEQbF0F8H3LZDmhDbywgSUiJSt582sOu23A=s64","userId":"07106977847402479880"}},"outputId":"a3ffe529-0a64-426f-c87e-4b980698597e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /tmp/tmp9b95mlj5/assets\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"]},{"output_type":"stream","name":"stdout","text":["Saved quanted TFLite model to: /tmp/tmppxz67qcq.tflite\n"]}]},{"cell_type":"markdown","source":["**Check model size**"],"metadata":{"id":"ElqLk6qvltpB"}},{"cell_type":"code","source":["def get_gzipped_model_size(file):\n","  # Returns size of gzipped model, in bytes.\n","  import tempfile\n","  import os\n","  import zipfile\n","\n","  _, zipped_file = tempfile.mkstemp('.zip')\n","  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n","    f.write(file)\n","\n","  return os.path.getsize(zipped_file)"],"metadata":{"id":"1cgMghmZlyaD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size('/content/drive/Shareddrives/AAML_final/FireNet-LightWeight-Network-for-Fire-Detection/Trained_Model_FIreNet/train_nonpruning.h5')))\n","#print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size('/content/drive/Shareddrives/AAML_final/FireNet-LightWeight-Network-for-Fire-Detection/Trained_Model_FIreNet/all_prun_0_80.h5')))\n","#print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size('/content/drive/Shareddrives/AAML_final/FireNet-LightWeight-Network-for-Fire-Detection/Trained_Model_FIreNet/all_prun_0_80.tflite')))\n","\n","print(\"Size of gzipped post-training TFlite model: %.2f bytes\" % (get_gzipped_model_size('/content/drive/Shareddrives/AAML_final/FireNet-LightWeight-Network-for-Fire-Detection/Trained_Model_FIreNet/quant_part/post_training_quant.tflite')))\n","print(\"Size of gzipped 8bitDense TFlite model: %.2f bytes\" % (get_gzipped_model_size('/content/drive/Shareddrives/AAML_final/FireNet-LightWeight-Network-for-Fire-Detection/Trained_Model_FIreNet/quant_part/8bitDense.tflite')))\n","print(\"Size of gzipped 8bitConv2D TFlite model: %.2f bytes\" % (get_gzipped_model_size('/content/drive/Shareddrives/AAML_final/FireNet-LightWeight-Network-for-Fire-Detection/Trained_Model_FIreNet/quant_part/8bitConv2D.tflite')))\n","print(\"Size of gzipped quant_aware_default TFlite model: %.2f bytes\" % (get_gzipped_model_size('/content/drive/Shareddrives/AAML_final/FireNet-LightWeight-Network-for-Fire-Detection/Trained_Model_FIreNet/quant_part/quant_aware_default.tflite')))\n","print(\"Size of gzipped 8bitDense_pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size('/content/drive/Shareddrives/AAML_final/FireNet-LightWeight-Network-for-Fire-Detection/Trained_Model_FIreNet/quant_part/8bitDense_pruned.tflite')))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7wZ_0pbl5qV","executionInfo":{"status":"ok","timestamp":1641885019711,"user_tz":-480,"elapsed":2765,"user":{"displayName":"葉昭宏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggn_HbAEQbF0F8H3LZDmhDbywgSUiJSt582sOu23A=s64","userId":"07106977847402479880"}},"outputId":"9639b5b7-7e3d-4b94-a7c0-0441434c2080"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of gzipped baseline Keras model: 7061894.00 bytes\n","Size of gzipped post-training TFlite model: 529885.00 bytes\n","Size of gzipped 8bitDense TFlite model: 815270.00 bytes\n","Size of gzipped 8bitConv2D TFlite model: 2352752.00 bytes\n","Size of gzipped quant_aware_default TFlite model: 761781.00 bytes\n","Size of gzipped 8bitDense_pruned TFlite model: 982336.00 bytes\n"]}]}]}